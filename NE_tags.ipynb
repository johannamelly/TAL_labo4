{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((('Ahab', 'NNP'), 'PERSON'), 92),\n",
       " ((('Queequeg', 'NNP'), 'PERSON'), 87),\n",
       " ((('Captain', 'NNP'), 'PERSON'), 75),\n",
       " ((('Stubb', 'NNP'), 'PERSON'), 51),\n",
       " ((('Queequeg', 'NNP'), 'GPE'), 47),\n",
       " ((('Jonah', 'NNP'), 'PERSON'), 41),\n",
       " ((('Bildad', 'NNP'), 'GPE'), 36),\n",
       " ((('Starbuck', 'NNP'), 'PERSON'), 36),\n",
       " ((('Peleg', 'NNP'), 'PERSON'), 35),\n",
       " ((('White', 'NNP'), 'FACILITY'), 33),\n",
       " ((('Pequod', 'NNP'), 'ORGANIZATION'), 32),\n",
       " ((('Captain', 'NNP'), 'GPE'), 32),\n",
       " ((('Moby', 'NNP'), 'PERSON'), 28),\n",
       " ((('Nantucket', 'NNP'), 'GPE'), 27),\n",
       " ((('Flask', 'NNP'), 'PERSON'), 26),\n",
       " ((('Indian', 'JJ'), 'GPE'), 25),\n",
       " ((('New', 'NNP'), 'GPE'), 25),\n",
       " ((('Ahab', 'NNP'), 'GPE'), 23),\n",
       " ((('Mr.', 'NNP'), 'PERSON'), 23),\n",
       " ((('God', 'NNP'), 'PERSON'), 23),\n",
       " ((('Pequod', 'NNP'), 'GPE'), 22),\n",
       " ((('Bildad', 'NNP'), 'PERSON'), 22),\n",
       " ((('Greenland', 'NNP'), 'GPE'), 21),\n",
       " ((('Sperm', 'NNP'), 'ORGANIZATION'), 19),\n",
       " ((('Flask', 'NNP'), 'GPE'), 17),\n",
       " ((('American', 'JJ'), 'GPE'), 13),\n",
       " ((('Ishmael', 'NNP'), 'GPE'), 12),\n",
       " ((('Jonah', 'NNP'), 'GPE'), 12),\n",
       " ((('Yojo', 'NNP'), 'PERSON'), 11),\n",
       " ((('Pacific', 'NNP'), 'ORGANIZATION'), 11),\n",
       " ((('BOOK', 'NNP'), 'ORGANIZATION'), 11),\n",
       " ((('Tashtego', 'NNP'), 'PERSON'), 10),\n",
       " ((('CHAPTER', 'NNP'), 'ORGANIZATION'), 10),\n",
       " ((('English', 'NNP'), 'GPE'), 10),\n",
       " ((('Tashtego', 'NNP'), 'GPE'), 9),\n",
       " ((('New', 'NNP'), 'ORGANIZATION'), 9),\n",
       " ((('Elijah', 'NNP'), 'PERSON'), 9),\n",
       " ((('Lord', 'NNP'), 'ORGANIZATION'), 9),\n",
       " ((('Queequeg', 'NNP'), 'ORGANIZATION'), 9),\n",
       " ((('Starbuck', 'NNP'), 'GPE'), 9),\n",
       " ((('Commodore', 'NNP'), 'ORGANIZATION'), 9),\n",
       " ((('Don', 'NNP'), 'PERSON'), 9),\n",
       " ((('Nantucket', 'NNP'), 'PERSON'), 9),\n",
       " ((('Cape', 'NNP'), 'PERSON'), 8),\n",
       " ((('Whale', 'NNP'), 'ORGANIZATION'), 8),\n",
       " ((('Leviathan', 'NNP'), 'PERSON'), 8),\n",
       " ((('Aye', 'NNP'), 'GPE'), 8),\n",
       " ((('Father', 'NNP'), 'PERSON'), 8),\n",
       " ((('Stubb', 'NNP'), 'GPE'), 8),\n",
       " ((('Atlantic', 'NNP'), 'ORGANIZATION'), 8)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os, codecs\n",
    "from urllib import request\n",
    "import dateparser\n",
    "\n",
    "\n",
    "\n",
    "from urllib import request\n",
    "url1 = \"http://www.gutenberg.org/files/2701/2701-0.txt\" # pick a text here\n",
    "# Please write your Python code below and execute it.\n",
    "response = request.urlopen(url1)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "start = raw.find(\"MOBY-DICK;\")\n",
    "end = raw.rfind(\"End of Project Gutenbergâ€™s Moby Dick\")\n",
    "raw = raw[start:end]\n",
    "len(raw)\n",
    "NEs = []\n",
    "labeledNEs = []\n",
    "sentences = []\n",
    "for sent in nltk.sent_tokenize(raw[:500000]):\n",
    "    sentences.append(sent)\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            NEs.append(chunk[0])\n",
    "            labeledNEs.append((chunk[0],chunk.label()))\n",
    "\n",
    "frequency = nltk.FreqDist(labeledNEs)\n",
    "frequency.most_common(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Queequeg', 'LOCATION'), 165)\n",
      "(('Ahab', 'PERSON'), 163)\n",
      "(('Peleg', 'PERSON'), 67)\n",
      "(('Bildad', 'PERSON'), 58)\n",
      "(('Jonah', 'PERSON'), 58)\n",
      "(('Nantucket', 'LOCATION'), 50)\n",
      "(('Starbuck', 'PERSON'), 36)\n",
      "(('Stubb', 'ORGANIZATION'), 35)\n",
      "(('Dick', 'PERSON'), 33)\n",
      "(('Stubb', 'PERSON'), 31)\n",
      "(('New', 'LOCATION'), 29)\n",
      "(('Pequod', 'LOCATION'), 24)\n",
      "(('Greenland', 'LOCATION'), 21)\n",
      "(('White', 'LOCATION'), 17)\n",
      "(('Pacific', 'LOCATION'), 16)\n",
      "(('Hussey', 'PERSON'), 16)\n",
      "(('Whale', 'LOCATION'), 14)\n",
      "(('Ishmael', 'PERSON'), 14)\n",
      "(('Elijah', 'PERSON'), 13)\n",
      "(('Starbuck', 'ORGANIZATION'), 13)\n",
      "(('Daggoo', 'PERSON'), 13)\n",
      "(('Cape', 'LOCATION'), 12)\n",
      "(('Bedford', 'LOCATION'), 12)\n",
      "(('Tashtego', 'PERSON'), 11)\n",
      "(('Atlantic', 'LOCATION'), 9)\n",
      "(('Bildad', 'LOCATION'), 9)\n",
      "(('Yojo', 'PERSON'), 9)\n",
      "(('Mapple', 'PERSON'), 8)\n",
      "(('England', 'LOCATION'), 8)\n",
      "(('Africa', 'LOCATION'), 6)\n",
      "(('Sea', 'LOCATION'), 6)\n",
      "(('Japan', 'LOCATION'), 6)\n",
      "(('Whale', 'ORGANIZATION'), 5)\n",
      "(('Mediterranean', 'LOCATION'), 5)\n",
      "(('Zealand', 'LOCATION'), 5)\n",
      "(('Hosea', 'PERSON'), 5)\n",
      "(('America', 'LOCATION'), 5)\n",
      "(('London', 'LOCATION'), 5)\n",
      "(('Mountains', 'LOCATION'), 5)\n",
      "(('Peter', 'PERSON'), 5)\n",
      "(('Coffin', 'PERSON'), 5)\n",
      "(('Horn', 'PERSON'), 5)\n",
      "(('Cabaco', 'PERSON'), 5)\n",
      "(('Horn', 'LOCATION'), 5)\n",
      "(('Quaker', 'ORGANIZATION'), 5)\n",
      "(('Cape', 'PERSON'), 5)\n",
      "(('Cadiz', 'LOCATION'), 4)\n",
      "(('of', 'LOCATION'), 4)\n",
      "(('South', 'LOCATION'), 4)\n",
      "(('Louis', 'PERSON'), 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Queequeg', 'LOCATION'), 165),\n",
       " (('Ahab', 'PERSON'), 163),\n",
       " (('Peleg', 'PERSON'), 67),\n",
       " (('Bildad', 'PERSON'), 58),\n",
       " (('Jonah', 'PERSON'), 58),\n",
       " (('Nantucket', 'LOCATION'), 50),\n",
       " (('Starbuck', 'PERSON'), 36),\n",
       " (('Stubb', 'ORGANIZATION'), 35),\n",
       " (('Dick', 'PERSON'), 33),\n",
       " (('Stubb', 'PERSON'), 31),\n",
       " (('New', 'LOCATION'), 29),\n",
       " (('Pequod', 'LOCATION'), 24),\n",
       " (('Greenland', 'LOCATION'), 21),\n",
       " (('White', 'LOCATION'), 17),\n",
       " (('Pacific', 'LOCATION'), 16),\n",
       " (('Hussey', 'PERSON'), 16),\n",
       " (('Whale', 'LOCATION'), 14),\n",
       " (('Ishmael', 'PERSON'), 14),\n",
       " (('Elijah', 'PERSON'), 13),\n",
       " (('Starbuck', 'ORGANIZATION'), 13),\n",
       " (('Daggoo', 'PERSON'), 13),\n",
       " (('Cape', 'LOCATION'), 12),\n",
       " (('Bedford', 'LOCATION'), 12),\n",
       " (('Tashtego', 'PERSON'), 11),\n",
       " (('Atlantic', 'LOCATION'), 9),\n",
       " (('Bildad', 'LOCATION'), 9),\n",
       " (('Yojo', 'PERSON'), 9),\n",
       " (('Mapple', 'PERSON'), 8),\n",
       " (('England', 'LOCATION'), 8),\n",
       " (('Africa', 'LOCATION'), 6),\n",
       " (('Sea', 'LOCATION'), 6),\n",
       " (('Japan', 'LOCATION'), 6),\n",
       " (('Whale', 'ORGANIZATION'), 5),\n",
       " (('Mediterranean', 'LOCATION'), 5),\n",
       " (('Zealand', 'LOCATION'), 5),\n",
       " (('Hosea', 'PERSON'), 5),\n",
       " (('America', 'LOCATION'), 5),\n",
       " (('London', 'LOCATION'), 5),\n",
       " (('Mountains', 'LOCATION'), 5),\n",
       " (('Peter', 'PERSON'), 5),\n",
       " (('Coffin', 'PERSON'), 5),\n",
       " (('Horn', 'PERSON'), 5),\n",
       " (('Cabaco', 'PERSON'), 5),\n",
       " (('Horn', 'LOCATION'), 5),\n",
       " (('Quaker', 'ORGANIZATION'), 5),\n",
       " (('Cape', 'PERSON'), 5),\n",
       " (('Cadiz', 'LOCATION'), 4),\n",
       " (('of', 'LOCATION'), 4),\n",
       " (('South', 'LOCATION'), 4),\n",
       " (('Louis', 'PERSON'), 4)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "words = []\n",
    "for sent in sentences:\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        words.append(word)\n",
    "\n",
    "nerfile = '../stanford-ner-2018-10-16/stanford-ner.jar'\n",
    "modelfile = '../stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=modelfile, path_to_jar=nerfile)\n",
    "\n",
    "NEs2 = []\n",
    "\n",
    "for taggedWords in st.tag(words):\n",
    "    if taggedWords[1] != 'O':\n",
    "        NEs2.append(taggedWords)\n",
    "        \n",
    "        \n",
    "frequencyLabeledNltkNE = nltk.FreqDist(NEs)\n",
    "for i in frequencyLabeledNltkNE.most_common(50):\n",
    "    print(i)\n",
    "                       \n",
    "frequencyLabeledStanfordNE = nltk.FreqDist(NEs2)\n",
    "frequencyLabeledStanfordNE.most_common(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
