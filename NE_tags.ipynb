{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Queequeg', 'NNP'), 143),\n",
       " (('Ahab', 'NNP'), 119),\n",
       " (('Captain', 'NNP'), 110),\n",
       " (('Stubb', 'NNP'), 59),\n",
       " (('Bildad', 'NNP'), 59),\n",
       " (('Pequod', 'NNP'), 56),\n",
       " (('Jonah', 'NNP'), 53),\n",
       " (('Starbuck', 'NNP'), 46),\n",
       " (('Flask', 'NNP'), 44),\n",
       " (('Peleg', 'NNP'), 39),\n",
       " (('Nantucket', 'NNP'), 39),\n",
       " (('White', 'NNP'), 35),\n",
       " (('New', 'NNP'), 34),\n",
       " (('Moby', 'NNP'), 33),\n",
       " (('God', 'NNP'), 31),\n",
       " (('Indian', 'JJ'), 25),\n",
       " (('Sperm', 'NNP'), 23),\n",
       " (('Mr.', 'NNP'), 23),\n",
       " (('Greenland', 'NNP'), 22),\n",
       " (('Tashtego', 'NNP'), 19),\n",
       " (('Pacific', 'NNP'), 16),\n",
       " (('Aye', 'NNP'), 15),\n",
       " (('American', 'JJ'), 15),\n",
       " (('Ishmael', 'NNP'), 14),\n",
       " (('Whale', 'NNP'), 14),\n",
       " (('Lord', 'NNP'), 13),\n",
       " (('Cape', 'NNP'), 13),\n",
       " (('Leviathan', 'NNP'), 12),\n",
       " (('Daggoo', 'NNP'), 12),\n",
       " (('Yojo', 'NNP'), 11),\n",
       " (('English', 'NNP'), 11),\n",
       " (('BOOK', 'NNP'), 11),\n",
       " (('Elijah', 'NNP'), 10),\n",
       " (('CHAPTER', 'NNP'), 10),\n",
       " (('Commodore', 'NNP'), 9),\n",
       " (('Don', 'NNP'), 9),\n",
       " (('Ramadan', 'NNP'), 9),\n",
       " (('Atlantic', 'NNP'), 9),\n",
       " (('Sir', 'NNP'), 8),\n",
       " (('O', 'NNP'), 8),\n",
       " (('Father', 'NNP'), 8),\n",
       " (('Old', 'NNP'), 8),\n",
       " (('Christian', 'NNP'), 8),\n",
       " (('Quohog', 'NNP'), 8),\n",
       " (('Black', 'NNP'), 7),\n",
       " (('Cetology', 'NNP'), 7),\n",
       " (('_Folio_', 'NN'), 7),\n",
       " (('_Octavo_', 'NN'), 7),\n",
       " (('Death', 'NNP'), 7),\n",
       " (('Great', 'NNP'), 6)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os, codecs\n",
    "from urllib import request\n",
    "import dateparser\n",
    "\n",
    "\n",
    "\n",
    "from urllib import request\n",
    "url1 = \"http://www.gutenberg.org/files/2701/2701-0.txt\" # pick a text here\n",
    "# Please write your Python code below and execute it.\n",
    "response = request.urlopen(url1)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "start = raw.find(\"MOBY-DICK;\")\n",
    "end = raw.rfind(\"End of Project Gutenbergâ€™s Moby Dick\")\n",
    "raw = raw[start:end]\n",
    "len(raw)\n",
    "NEs = []\n",
    "labeledNEs = []\n",
    "sentences = []\n",
    "for sent in nltk.sent_tokenize(raw[:500000]):\n",
    "    sentences.append(sent)\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            NEs.append(chunk[0])\n",
    "            labeledNEs.append((chunk[0],chunk.label()))\n",
    "\n",
    "frequency = nltk.FreqDist(NEs)\n",
    "frequency.most_common(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((('Ahab', 'NNP'), 'PERSON'), 92)\n",
      "((('Queequeg', 'NNP'), 'PERSON'), 87)\n",
      "((('Captain', 'NNP'), 'PERSON'), 75)\n",
      "((('Stubb', 'NNP'), 'PERSON'), 51)\n",
      "((('Queequeg', 'NNP'), 'GPE'), 47)\n",
      "((('Jonah', 'NNP'), 'PERSON'), 41)\n",
      "((('Bildad', 'NNP'), 'GPE'), 36)\n",
      "((('Starbuck', 'NNP'), 'PERSON'), 36)\n",
      "((('Peleg', 'NNP'), 'PERSON'), 35)\n",
      "((('White', 'NNP'), 'FACILITY'), 33)\n",
      "((('Pequod', 'NNP'), 'ORGANIZATION'), 32)\n",
      "((('Captain', 'NNP'), 'GPE'), 32)\n",
      "((('Moby', 'NNP'), 'PERSON'), 28)\n",
      "((('Nantucket', 'NNP'), 'GPE'), 27)\n",
      "((('Flask', 'NNP'), 'PERSON'), 26)\n",
      "((('Indian', 'JJ'), 'GPE'), 25)\n",
      "((('New', 'NNP'), 'GPE'), 25)\n",
      "((('Ahab', 'NNP'), 'GPE'), 23)\n",
      "((('Mr.', 'NNP'), 'PERSON'), 23)\n",
      "((('God', 'NNP'), 'PERSON'), 23)\n",
      "((('Pequod', 'NNP'), 'GPE'), 22)\n",
      "((('Bildad', 'NNP'), 'PERSON'), 22)\n",
      "((('Greenland', 'NNP'), 'GPE'), 21)\n",
      "((('Sperm', 'NNP'), 'ORGANIZATION'), 19)\n",
      "((('Flask', 'NNP'), 'GPE'), 17)\n",
      "((('American', 'JJ'), 'GPE'), 13)\n",
      "((('Ishmael', 'NNP'), 'GPE'), 12)\n",
      "((('Jonah', 'NNP'), 'GPE'), 12)\n",
      "((('Yojo', 'NNP'), 'PERSON'), 11)\n",
      "((('Pacific', 'NNP'), 'ORGANIZATION'), 11)\n",
      "((('BOOK', 'NNP'), 'ORGANIZATION'), 11)\n",
      "((('Tashtego', 'NNP'), 'PERSON'), 10)\n",
      "((('CHAPTER', 'NNP'), 'ORGANIZATION'), 10)\n",
      "((('English', 'NNP'), 'GPE'), 10)\n",
      "((('Tashtego', 'NNP'), 'GPE'), 9)\n",
      "((('New', 'NNP'), 'ORGANIZATION'), 9)\n",
      "((('Elijah', 'NNP'), 'PERSON'), 9)\n",
      "((('Lord', 'NNP'), 'ORGANIZATION'), 9)\n",
      "((('Queequeg', 'NNP'), 'ORGANIZATION'), 9)\n",
      "((('Starbuck', 'NNP'), 'GPE'), 9)\n",
      "((('Commodore', 'NNP'), 'ORGANIZATION'), 9)\n",
      "((('Don', 'NNP'), 'PERSON'), 9)\n",
      "((('Nantucket', 'NNP'), 'PERSON'), 9)\n",
      "((('Cape', 'NNP'), 'PERSON'), 8)\n",
      "((('Whale', 'NNP'), 'ORGANIZATION'), 8)\n",
      "((('Leviathan', 'NNP'), 'PERSON'), 8)\n",
      "((('Aye', 'NNP'), 'GPE'), 8)\n",
      "((('Father', 'NNP'), 'PERSON'), 8)\n",
      "((('Stubb', 'NNP'), 'GPE'), 8)\n",
      "((('Atlantic', 'NNP'), 'ORGANIZATION'), 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Queequeg', 'LOCATION'), 165),\n",
       " (('Ahab', 'PERSON'), 163),\n",
       " (('Peleg', 'PERSON'), 67),\n",
       " (('Bildad', 'PERSON'), 58),\n",
       " (('Jonah', 'PERSON'), 58),\n",
       " (('Nantucket', 'LOCATION'), 50),\n",
       " (('Starbuck', 'PERSON'), 36),\n",
       " (('Stubb', 'ORGANIZATION'), 35),\n",
       " (('Dick', 'PERSON'), 33),\n",
       " (('Stubb', 'PERSON'), 31),\n",
       " (('New', 'LOCATION'), 29),\n",
       " (('Pequod', 'LOCATION'), 24),\n",
       " (('Greenland', 'LOCATION'), 21),\n",
       " (('White', 'LOCATION'), 17),\n",
       " (('Pacific', 'LOCATION'), 16),\n",
       " (('Hussey', 'PERSON'), 16),\n",
       " (('Whale', 'LOCATION'), 14),\n",
       " (('Ishmael', 'PERSON'), 14),\n",
       " (('Elijah', 'PERSON'), 13),\n",
       " (('Starbuck', 'ORGANIZATION'), 13),\n",
       " (('Daggoo', 'PERSON'), 13),\n",
       " (('Cape', 'LOCATION'), 12),\n",
       " (('Bedford', 'LOCATION'), 12),\n",
       " (('Tashtego', 'PERSON'), 11),\n",
       " (('Atlantic', 'LOCATION'), 9),\n",
       " (('Bildad', 'LOCATION'), 9),\n",
       " (('Yojo', 'PERSON'), 9),\n",
       " (('Mapple', 'PERSON'), 8),\n",
       " (('England', 'LOCATION'), 8),\n",
       " (('Africa', 'LOCATION'), 6),\n",
       " (('Sea', 'LOCATION'), 6),\n",
       " (('Japan', 'LOCATION'), 6),\n",
       " (('Whale', 'ORGANIZATION'), 5),\n",
       " (('Mediterranean', 'LOCATION'), 5),\n",
       " (('Zealand', 'LOCATION'), 5),\n",
       " (('Hosea', 'PERSON'), 5),\n",
       " (('America', 'LOCATION'), 5),\n",
       " (('London', 'LOCATION'), 5),\n",
       " (('Mountains', 'LOCATION'), 5),\n",
       " (('Peter', 'PERSON'), 5),\n",
       " (('Coffin', 'PERSON'), 5),\n",
       " (('Horn', 'PERSON'), 5),\n",
       " (('Cabaco', 'PERSON'), 5),\n",
       " (('Horn', 'LOCATION'), 5),\n",
       " (('Quaker', 'ORGANIZATION'), 5),\n",
       " (('Cape', 'PERSON'), 5),\n",
       " (('Cadiz', 'LOCATION'), 4),\n",
       " (('of', 'LOCATION'), 4),\n",
       " (('South', 'LOCATION'), 4),\n",
       " (('Louis', 'PERSON'), 4)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "words = []\n",
    "for sent in sentences:\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        words.append(word)\n",
    "\n",
    "nerfile = '../stanford-ner-2018-10-16/stanford-ner.jar'\n",
    "modelfile = '../stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "\n",
    "st = StanfordNERTagger(model_filename=modelfile, path_to_jar=nerfile)\n",
    "\n",
    "NEs2 = []\n",
    "\n",
    "for taggedWords in st.tag(words):\n",
    "    if taggedWords[1] != 'O':\n",
    "        NEs2.append(taggedWords)\n",
    "        \n",
    "        \n",
    "frequencyLabeledNltkNE = nltk.FreqDist(labeledNEs)\n",
    "for i in frequencyLabeledNltkNE.most_common(50):\n",
    "    print(i)\n",
    "                       \n",
    "frequencyLabeledStanfordNE = nltk.FreqDist(NEs2)\n",
    "frequencyLabeledStanfordNE.most_common(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
